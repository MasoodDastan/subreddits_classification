{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f43c75-789b-48f9-8835-7b1086e72dc0",
   "metadata": {},
   "source": [
    "## Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7960464-26ea-4929-bbb2-dfb595a3d036",
   "metadata": {},
   "source": [
    "This notebook is intended for scraping data from the Reddit API. The project's objective is to collect data within a defined timeframe, specifically from June 13th to June 18th. The data is sourced from two subreddits: `Personal Finance` and `Investing`. As a result of recent restrictions and limitations imposed by Reddit, the data collection process needs to be executed on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6e851c-6820-497f-90bb-863c86b8b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install praw\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b4905-6fce-4f95-9818-55716c6902f6",
   "metadata": {},
   "source": [
    "I am utilizing PRAW, the official Python wrapper for Reddit's API, to access Reddit's API functionality. Before proceeding, I created a Reddit application and obtained the necessary API credentials from [here](https://www.reddit.com/prefs/apps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb8aa73-5e90-46fa-883b-4af0439e5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import API_KEY, API_SECRET, Reddit_password, Reddit_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d9b3f1-ba0b-42d9-9df2-082e250767e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= API_KEY,\n",
    "    client_secret= API_SECRET,\n",
    "    user_agent='praw',\n",
    "    username= Reddit_username,\n",
    "    password= Reddit_password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fc74e5-6f5c-4177-9c62-373886228c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(posts, label):\n",
    "    \"\"\"\n",
    "    Combine relevant information from Reddit posts into a list of data rows.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of Reddit post objects.\n",
    "        label (str): Label or category associated with the posts.\n",
    "\n",
    "    Returns:\n",
    "        list: List of data rows, each containing the post's creation time, title, selftext, and subreddit.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []  # List to store the combined data rows\n",
    "\n",
    "    for p in posts:\n",
    "        if p.stickied:\n",
    "            continue  # Skip stickied posts and move to the next iteration\n",
    "        else:\n",
    "            row = (p.created_utc, p.title, p.selftext, p.subreddit)  # Create a data row tuple\n",
    "            data.append(row)  # Append the row to the data list\n",
    "\n",
    "    min_time = int(min(r[0] for r in data)) - 100_000  # Calculate the minimum creation time in the data\n",
    "\n",
    "    print(f\"{label.upper()} Posts:: N= {len(data)}\")  # Print the number of posts processed\n",
    "    return data  # Return the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b990a8b-eddf-4230-9c3c-87a72d0cade4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230618'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c317da-8c2e-42c3-9e98-5a915abb2bcb",
   "metadata": {},
   "source": [
    "## Personal Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4399bd2f-4ba5-4982-a09a-56be34dd5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('PersonalFinance') # Get the 'PersonalFinance' subreddit instance\n",
    "\n",
    "posts_new = subreddit.new(limit=1000) # Get a listing generator for the newest 1000 posts\n",
    "posts_hot = subreddit.hot(limit=1000)\n",
    "\n",
    "posts_top_all = subreddit.top(limit=1000)\n",
    "posts_top_year = subreddit.top(limit=1000, time_filter=\"year\")\n",
    "posts_top_month = subreddit.top(limit=1000, time_filter=\"month\")\n",
    "posts_top_week = subreddit.top(limit=1000, time_filter=\"week\")\n",
    "\n",
    "posts_con_all = subreddit.controversial(limit=1000)\n",
    "posts_con_year = subreddit.controversial(limit=1000, time_filter=\"year\")\n",
    "posts_con_month = subreddit.controversial(limit=1000, time_filter=\"month\")\n",
    "posts_con_week = subreddit.controversial(limit=1000, time_filter=\"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc0a9f1-df72-42fa-8a12-0838c79d8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW Posts:: N= 991\n",
      "HOT Posts:: N= 892\n",
      "sleeping for 60 seconds\n",
      "TOP_ALL Posts:: N= 990\n",
      "TOP_YEAR Posts:: N= 1000\n",
      "TOP_MONTH Posts:: N= 995\n",
      "TOP_WEEK Posts:: N= 996\n",
      "sleeping for another 60 seconds (last time)\n",
      "CONTROVERSIAL_ALL Posts:: N= 991\n",
      "CONTROVERSIAL_YEAR Posts:: N= 999\n",
      "CONTROVERSIAL_MONTH Posts:: N= 993\n",
      "CONTROVERSIAL_WEEK Posts:: N= 996\n"
     ]
    }
   ],
   "source": [
    "data_new = combine_data(posts_new, 'new')\n",
    "data_hot = combine_data(posts_hot, 'hot')\n",
    "\n",
    "print('sleeping for 60 seconds')\n",
    "# This is where I found how to add a wait time in my code --> https://realpython.com/python-sleep/\n",
    "time.sleep(60)\n",
    "\n",
    "data_top_all = combine_data(posts_top_all, 'top_all')\n",
    "data_top_year = combine_data(posts_top_year, 'top_year')\n",
    "data_top_month = combine_data(posts_top_month, 'top_month')\n",
    "data_top_week = combine_data(posts_top_week, 'top_week')\n",
    "\n",
    "print('sleeping for another 60 seconds (last time)')\n",
    "time.sleep(60)\n",
    "\n",
    "data_con_all = combine_data(posts_con_all, 'controversial_all')\n",
    "data_con_year = combine_data(posts_con_year, 'controversial_year')\n",
    "data_con_month = combine_data(posts_con_month, 'controversial_month')\n",
    "data_con_week = combine_data(posts_con_week, 'controversial_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5076dd6b-aed1-41b8-8855-66ef7f4c6c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6161, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_fiannce = pd.DataFrame(\n",
    "    data_new +\n",
    "    data_hot +\n",
    "    data_top_all +\n",
    "    data_top_year +\n",
    "    data_top_month +\n",
    "    data_top_week +\n",
    "    data_con_all +\n",
    "    data_con_year +\n",
    "    data_con_month +\n",
    "    data_con_week, \n",
    "    columns=['time', 'title', 'text', 'subreddit'])\n",
    "\n",
    "personal_fiannce = personal_fiannce.drop_duplicates()\n",
    "personal_fiannce.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee3cd67-cb53-4389-94b8-ad17f2e1884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_fiannce.to_csv(f\"../data/{today}-personalfiannce-praw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ad361-c6d3-4b46-b7ef-645d99a682a5",
   "metadata": {},
   "source": [
    "## Investing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b72579d-64fd-4aec-9fee-4f04aee048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('investing')\n",
    "\n",
    "posts_new = subreddit.new(limit=1000)\n",
    "posts_hot = subreddit.hot(limit=1000)\n",
    "\n",
    "posts_top_all = subreddit.top(limit=1000)\n",
    "posts_top_year = subreddit.top(limit=1000, time_filter=\"year\")\n",
    "posts_top_month = subreddit.top(limit=1000, time_filter=\"month\")\n",
    "posts_top_week = subreddit.top(limit=1000, time_filter=\"week\")\n",
    "\n",
    "posts_con_all = subreddit.controversial(limit=1000)\n",
    "posts_con_year = subreddit.controversial(limit=1000, time_filter=\"year\")\n",
    "posts_con_month = subreddit.controversial(limit=1000, time_filter=\"month\")\n",
    "posts_con_week = subreddit.controversial(limit=1000, time_filter=\"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2621389-23f2-4a30-a3fb-060793ba816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW Posts:: N= 845\n",
      "HOT Posts:: N= 376\n",
      "sleeping for 60 seconds\n",
      "TOP_ALL Posts:: N= 990\n",
      "TOP_YEAR Posts:: N= 1000\n",
      "TOP_MONTH Posts:: N= 567\n",
      "TOP_WEEK Posts:: N= 161\n",
      "sleeping for another 60 seconds (last time)\n",
      "CONTROVERSIAL_ALL Posts:: N= 993\n",
      "CONTROVERSIAL_YEAR Posts:: N= 1000\n",
      "CONTROVERSIAL_MONTH Posts:: N= 567\n",
      "CONTROVERSIAL_WEEK Posts:: N= 161\n"
     ]
    }
   ],
   "source": [
    "data_new = combine_data(posts_new, 'new')\n",
    "data_hot = combine_data(posts_hot, 'hot')\n",
    "\n",
    "print('sleeping for 60 seconds')\n",
    "time.sleep(60)\n",
    "\n",
    "data_top_all = combine_data(posts_top_all, 'top_all')\n",
    "data_top_year = combine_data(posts_top_year, 'top_year')\n",
    "data_top_month = combine_data(posts_top_month, 'top_month')\n",
    "data_top_week = combine_data(posts_top_week, 'top_week')\n",
    "\n",
    "print('sleeping for another 60 seconds (last time)')\n",
    "time.sleep(60)\n",
    "\n",
    "data_con_all = combine_data(posts_con_all, 'controversial_all')\n",
    "data_con_year = combine_data(posts_con_year, 'controversial_year')\n",
    "data_con_month = combine_data(posts_con_month, 'controversial_month')\n",
    "data_con_week = combine_data(posts_con_week, 'controversial_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9404e8-96ac-4b8d-95cd-74014c59a34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4296, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing = pd.DataFrame(\n",
    "    data_new +\n",
    "    data_hot +\n",
    "    data_top_all +\n",
    "    data_top_year +\n",
    "    data_top_month +\n",
    "    data_top_week +\n",
    "    data_con_all +\n",
    "    data_con_year +\n",
    "    data_con_month +\n",
    "    data_con_week, \n",
    "    columns=['time', 'title', 'text', 'subreddit'])\n",
    "\n",
    "investing = investing.drop_duplicates()\n",
    "investing.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ce6355-20d0-4f9b-a000-2db8ef09efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "investing.to_csv(f\"../data/{today}-investing-praw.csv\", \n",
    "                 index=False,\n",
    "                 escapechar= '\\\\' # Jeff Alexander helped me figure out the need for the escapechar                \n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
